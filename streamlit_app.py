# import streamlit as st
# from langchain.embeddings.openai import OpenAIEmbeddings
# from langchain.vectorstores import Chroma
# from langchain.text_splitter import CharacterTextSplitter
# from langchain.chains import ConversationalRetrievalChain
# from langchain.chat_models import ChatOpenAI
# from langchain.document_loaders import UnstructuredPDFLoader
# import os

# # Nastaven√≠ API kl√≠ƒçe prost≈ôednictv√≠m environment√°ln√≠ promƒõnn√©
# os.environ["OPENAI_API_KEY"] = st.secrets["OPENAI_API_KEY"]

# # Inicializace language modelu s specifikovan√Ωmi parametry
# llm = ChatOpenAI(temperature=0, max_tokens=1000, model_name="gpt-4-turbo")

# st.title("ü§ñKognitivn√≠ vyhled√°v√°n√≠ v obsahu dokumet≈Ø.")

# # Informaƒçn√≠ sekce s p≈ô√≠klady pou≈æit√≠
# st.info(
#     """
#     P≈ô√≠klady u≈æit√≠:\n
#     -Existuje program speci√°lnƒõ pro hedv√°b√≠?\n
#     -≈òekni mi vic o programu hedv√°b√≠\n
#     ...\n
#     """,
#     icon="üïµÔ∏è‚Äç‚ôÄÔ∏è",
# )

# # Sidebar pro nahr√°v√°n√≠ soubor≈Ø
# with st.sidebar:
#     uploaded_files = st.file_uploader("V√Ωbƒõr soubor≈Ø PDF", accept_multiple_files=True, type="pdf")

# if uploaded_files:
#     if "processed_data" not in st.session_state:
#         documents = []
#         for uploaded_file in uploaded_files:
#             file_path = os.path.join(os.getcwd(), uploaded_file.name)
#             with open(file_path, "wb") as f:
#                 f.write(uploaded_file.getvalue())

#             loader = UnstructuredPDFLoader(file_path)
#             loaded_documents = loader.load()
#             documents.extend(loaded_documents)

#         text_splitter = CharacterTextSplitter(chunk_size=2000, chunk_overlap=200)
#         document_chunks = text_splitter.split_documents(documents)

#         embeddings = OpenAIEmbeddings()
#         vectorstore = Chroma.from_documents(document_chunks, embeddings)

#         st.session_state.processed_data = {
#             "document_chunks": document_chunks,
#             "vectorstore": vectorstore,
#         }
#     else:
#         document_chunks = st.session_state.processed_data["document_chunks"]
#         vectorstore = st.session_state.processed_data["vectorstore"]

#     qa = ConversationalRetrievalChain.from_llm(llm, vectorstore.as_retriever(), return_source_documents=True)

#     if "messages" not in st.session_state:
#         st.session_state.messages = []

#     for message in st.session_state.messages:
#         with st.chat_message(message["role"]):
#             st.markdown(message["content"])

#     if prompt := st.chat_input("ü§ñCo chcete vƒõdƒõt?"):
#         st.session_state.messages.append({"role": "user", "content": prompt})
#         with st.chat_message("user"):
#             st.markdown(prompt)

#         # Konsolidace zpr√°v pro sn√≠≈æen√≠ poƒçtu API po≈æadavk≈Ø
#         chat_history = [(message["role"], message["content"]) for message in st.session_state.messages if message["role"] != "system"]

#         result = qa({"question": prompt, "chat_history": chat_history})

#         source_documents = result.get('source_documents', [])
#         document_attributes = [vars(doc) for doc in source_documents]
#         file_names = [os.path.basename(doc["metadata"]["source"]) for doc in document_attributes if doc["metadata"]["source"] is not None]

#         file_names_string = ', '.join(file_names) if file_names else "Nejsou dostupn√© ≈æ√°dn√© soubory"

#         with st.chat_message("assistant"):
#             message_placeholder = st.empty()
#             full_response = result["answer"]
#             message_placeholder.markdown(full_response)
        
#         st.session_state.messages.append({"role": "assistant", "content": full_response})

#         formatted_text = ""
#         if source_documents:
#             page_content = [doc["page_content"] for doc in document_attributes]
#             formatted_text = page_content[0].replace('\n', ' ')

#         with st.expander("Zdrojov√Ω text pro odpovƒõƒè"):
#             st.write(formatted_text)
# else:
#     st.write("Pros√≠m nahrajte soubory PDF.")


from openai import OpenAI
import streamlit as st

st.title("Ofici√°lnƒõ podporovan√Ω a plnƒõ v souladu se v≈°emi p≈ôedpisy: GPT klon, kter√Ω nep≈ôekraƒçuje ≈æ√°dn√© stanovy ani pravidla Zscaleru")

client = OpenAI(api_key=st.secrets["sk-proj-KF8TKKi5FVeFfvd1g0ssT3BlbkFJnKESAChVOL5fMC5CdrXY"])

if "openai_model" not in st.session_state:
    st.session_state["openai_model"] = "gpt-3.5-turbo"

if "messages" not in st.session_state:
    st.session_state.messages = []

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

if prompt := st.chat_input("What is up?"):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        stream = client.chat.completions.create(
            model=st.session_state["openai_model"],
            messages=[
                {"role": m["role"], "content": m["content"]}
                for m in st.session_state.messages
            ],
            stream=True,
        )
        response = st.write_stream(stream)
    st.session_state.messages.append({"role": "assistant", "content": response})
