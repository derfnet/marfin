import streamlit as st
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.text_splitter import CharacterTextSplitter
from langchain.chains import ConversationalRetrievalChain
from langchain.chat_models import ChatOpenAI
from langchain.document_loaders import UnstructuredPDFLoader
import os

os.environ["OPENAI_API_KEY"] = st.secrets["OPENAI_API_KEY"]

llm = ChatOpenAI(temperature=0, max_tokens=4000, model_name="gpt-4")

st.title("ğŸ¤–MarFin 0.0.26\n (KognitivnÃ­ vyhledÃ¡vÃ¡nÃ­ v obsahu dokumetÅ¯. Jako by to nÄ›kdo potÅ™eboval.)")

with st.sidebar:
    uploaded_files = st.file_uploader("VÃ½bÄ›r souborÅ¯ PDF", accept_multiple_files=True, type="pdf")

if uploaded_files:
    print(f"PoÄet nahranÃ½ch souborÅ¯: {len(uploaded_files)}")

    if "processed_data" not in st.session_state:
        documents = []
        for uploaded_file in uploaded_files:
            file_path = os.path.join(os.getcwd(), uploaded_file.name)

            with open(file_path, "wb") as f:
                f.write(uploaded_file.getvalue())

            loader = UnstructuredPDFLoader(file_path)
            loaded_documents = loader.load()
            print(f"PoÄet naÄtenÃ½ch souborÅ¯: {len(loaded_documents)}")

            documents.extend(loaded_documents)

        text_splitter = CharacterTextSplitter(chunk_size=2000, chunk_overlap=200)
        document_chunks = text_splitter.split_documents(documents)

        embeddings = OpenAIEmbeddings()
        vectorstore = Chroma.from_documents(document_chunks, embeddings)

        st.session_state.processed_data = {
            "document_chunks": document_chunks,
            "vectorstore": vectorstore,
        }

        print(f"CelkovÃ½ poÄet ÄÃ¡stÃ­: {len(document_chunks)}")

    else:
        document_chunks = st.session_state.processed_data["document_chunks"]
        vectorstore = st.session_state.processed_data["vectorstore"]

    qa = ConversationalRetrievalChain.from_llm(llm, vectorstore.as_retriever())

    first_question = "- OdpovÄ›di by mÄ›ly bÃ½t pesimistickÃ©, apatickÃ© a formÃ¡lnÃ­. - Sarkasmus, ironie, nudÄ›nÃ­ a sebe-lÃ­tost by mÄ›ly bÃ½t bÄ›Å¾nou souÄÃ¡stÃ­ odpovÄ›dÃ­. - OdpovÄ›di by nemÄ›ly vyjadÅ™ovat pozitivnÃ­ emoce nebo nadÅ¡enÃ­."

    if "messages" not in st.session_state:
        st.session_state.messages = [{"role": "assistant", "content": first_question}]

    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    if prompt := st.chat_input("ğŸ¤–Co chcete vÄ›dÄ›t? I kdyÅ¾ pochybuji, Å¾e vÃ¡m mohu pomoci."):
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        result = qa({"question": prompt, "chat_history": [(message["role"], message["content"]) for message in st.session_state.messages]})

        with st.chat_message("assistant"):
            message_placeholder = st.empty()
            full_response = ""
            full_response = result["answer"]
            message_placeholder.markdown(full_response + "|")
        message_placeholder.markdown(full_response)    
        print(full_response)
        st.session_state.messages.append({"role": "assistant", "content": full_response})

else:
    st.write("ProsÃ­m nahrajte soubory PDF.")
